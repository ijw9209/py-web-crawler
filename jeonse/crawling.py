# import requests
# from bs4 import BeautifulSoup
# import pandas as pd
# from datetime import datetime

# # í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
# now = datetime.now().strftime("%Y%m%d_%H%M")

# # í¬ë¡¤ë§í•  URL
# url = 'https://www.khug.or.kr/jeonse/web/s07/s070102.jsp'

# # URLì— HTTP GET ìš”ì²­ ë³´ë‚´ê¸°
# response = requests.get(url)

# soup = BeautifulSoup(response.text, 'html.parser')

# # í˜ì´ì§€ ìˆ˜ íŒŒì•…í•˜ê¸° (HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
# # ì˜ˆë¥¼ ë“¤ì–´, í˜ì´ì§€ ë²ˆí˜¸ê°€ <span class="page-number">ë¡œ ë˜ì–´ ìˆë‹¤ë©´ í•´ë‹¹ ìš”ì†Œë¥¼ ì°¾ì•„ì„œ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ íŒŒì•…
# page_numbers = soup.find_all('span', class_='num')  # í´ë˜ìŠ¤ëª…ì€ ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
# last_page = 26 #int(page_numbers[-1].text)  # ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ

# # ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§í•˜ê¸°
# all_data = []

# for page in range(1, last_page + 1):
#     # í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€í•˜ì—¬ URL ìˆ˜ì • 01 ì„œìš¸ 07 ê²½ê¸°
#     page_url = f"{url}?sbGugun=ALL&CMB_SIDO=01&cur_page={page}"
#     # page_url = f"{url}?sbGugun=ALL&CMB_SIDO=07&cur_page={page}"
#     response = requests.get(page_url)
    
#     if response.status_code == 200:
#         soup = BeautifulSoup(response.text, 'html.parser')
        
#         # í…Œì´ë¸” ì°¾ê¸° (ì˜ˆì‹œë¡œ í…Œì´ë¸” ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•œë‹¤ê³  ê°€ì •)
#         table = soup.find('table')  # ì‹¤ì œ í…Œì´ë¸”ì„ ì°¾ëŠ” HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
#         rows = table.find_all('tr')
        
#         # ê° í–‰ì—ì„œ ë°ì´í„° ì¶”ì¶œ
#         for row in rows:
#             cols = row.find_all('td')
#             if len(cols) > 0:  # ë¹ˆ ë°°ì—´ì„ í•„í„°ë§
#                 cols = [col.text.strip() for col in cols]  # ë°ì´í„° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#                 all_data.append(cols)  # ëª¨ë“  ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    
#     else:
#         print(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨")


# # DataFrameìœ¼ë¡œ ë³€í™˜ (ì›í•˜ëŠ” ì—´ ì´ë¦„ì„ ë„£ì–´ì£¼ê¸°)
# df = pd.DataFrame(all_data, columns=["ë²ˆí˜¸", "ê³µê³ ì¼ì", "ì²­ì•½ ì ‘ìˆ˜ê¸°ê°„	", "ì‹œë„", "ì‹œêµ°êµ¬" ,"ì£¼ì†Œ", "ì£¼íƒìœ í˜•" , "ì „ìš©ë©´ì (m2)" , "ì„ëŒ€ë³´ì¦ê¸ˆì•¡" ,"ì‹ ì²­ììˆ˜"])  # ì‹¤ì œ í…Œì´ë¸”ì— ë§ëŠ” ì—´ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •

# # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
# # íŒŒì¼ ì´ë¦„ì— ë‚ ì§œì™€ ì‹œê°„ ì¶”ê°€
# file_name = f'í¬ë¡¤ë§_ë°ì´í„°{now}.xlsx'
# df.to_excel(file_name, index=False, engine='openpyxl')
# print(f"ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_name}")

# # ì „ì²´ ë°ì´í„° ì¶œë ¥ (ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬)
# for data in all_data:
#     print(data)

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
now = datetime.now().strftime("%Y%m%d_%H%M")

# ì§€ì—­ ì½”ë“œ ëª©ë¡ (ì„œìš¸: 01, ê²½ê¸°: 07 ë“±)
regions = {
    "ì„œìš¸": "01",
    "ê²½ê¸°": "07"
}

# í¬ë¡¤ë§í•  ê¸°ë³¸ URL
base_url = "https://www.khug.or.kr/jeonse/web/s07/s070102.jsp"

# ì§€ì—­ë³„ í¬ë¡¤ë§ ì‹¤í–‰
for region_name, sido_code in regions.items():
    all_data = []  # ê° ì§€ì—­ë³„ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    # ì²« ë²ˆì§¸ í˜ì´ì§€ ìš”ì²­ (ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°)
    first_page_url = f"{base_url}?sbGugun=ALL&CMB_SIDO={sido_code}&cur_page=1"
    response = requests.get(first_page_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # **ğŸ”¹ "ë§ˆì§€ë§‰ í˜ì´ì§€" ë²„íŠ¼ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°**
        last_page = 1  # ê¸°ë³¸ê°’ 1
        last_page_btn = soup.find("a", class_="last")  # "ë§ˆì§€ë§‰ í˜ì´ì§€" ë²„íŠ¼ ì°¾ê¸°

        if last_page_btn and "href" in last_page_btn.attrs:
            last_page_url = last_page_btn["href"]  # href ì†ì„± ê°’ ê°€ì ¸ì˜¤ê¸°
            last_page = int(last_page_url.split("cur_page=")[-1])  # cur_page ê°’ ì¶”ì¶œ

        print(f"{region_name} ì§€ì—­ í¬ë¡¤ë§ ì‹œì‘ (ì´ {last_page} í˜ì´ì§€)")

        # ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
        for page in range(1, last_page + 1):
            page_url = f"{base_url}?sbGugun=ALL&CMB_SIDO={sido_code}&cur_page={page}"
            response = requests.get(page_url)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                table = soup.find('table')

                if table:
                    rows = table.find_all('tr')

                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) > 0:
                            cols = [col.text.strip() for col in cols]
                            all_data.append(cols)
            else:
                print(f"{region_name} ì§€ì—­ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨")

    else:
        print(f"{region_name} ì§€ì—­ ì²« í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨")
        continue  # ë‹¤ìŒ ì§€ì—­ìœ¼ë¡œ ë„˜ì–´ê°

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(all_data, columns=["ë²ˆí˜¸", "ê³µê³ ì¼ì", "ì²­ì•½ ì ‘ìˆ˜ê¸°ê°„", "ì‹œë„", "ì‹œêµ°êµ¬", "ì£¼ì†Œ", "ì£¼íƒìœ í˜•", "ì „ìš©ë©´ì (m2)", "ì„ëŒ€ë³´ì¦ê¸ˆì•¡", "ì‹ ì²­ììˆ˜"])

    # íŒŒì¼ ì €ì¥ (ì§€ì—­ëª… í¬í•¨)
    file_name = f'í¬ë¡¤ë§_ë°ì´í„°_{region_name}_{now}.xlsx'
    df.to_excel(file_name, index=False, engine='openpyxl')

    print(f"{region_name} ì§€ì—­ í¬ë¡¤ë§ ì™„ë£Œ! ì—‘ì…€ ì €ì¥: {file_name}")

print("ëª¨ë“  ì§€ì—­ í¬ë¡¤ë§ ì™„ë£Œ!")
